- PhoGPT-7.5B-Fine-tuning-QLoRA
- Author: Mr.Jack _ C√¥ng ty www.BICweb.vn
- Start Date: 23 Dec-2023
- End Date: 05 Feb-2024

# Foundation:
- github: https://github.com/VinAIResearch/PhoGPT
- huggingface: https://huggingface.co/vinai/PhoGPT-7B5-Instruct
- PhoGPT: Generative Pre-training for Vietnamese (https://arxiv.org/abs/2311.02945)
- MPT model: https://huggingface.co/docs/transformers/model_doc/mpt ; https://www.mosaicml.com/mpt
- Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs (https://www.databricks.com/blog/mpt-7b)
- PEFT: https://huggingface.co/docs/peft/en/index ; https://arxiv.org/abs/2205.05638
- LoRA: https://huggingface.co/docs/diffusers/en/training/lora ; https://arxiv.org/abs/2106.09685
- Quantization: https://huggingface.co/docs/accelerate/en/usage_guides/quantization
- Flash attention: https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention ; https://arxiv.org/abs/2205.14135
- ALiBi: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (https://arxiv.org/abs/2108.12409)

# S·ª± ki·ªán:
- Ng√†y 21 Aug-2023: "V√†o ng√†y 21/08/2023, C√¥ng ty C·ªï ph·∫ßn VinBigdata c√¥ng b·ªë x√¢y d·ª±ng th√†nh c√¥ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ti·∫øng Vi·ªát, ƒë·∫∑t n·ªÅn m√≥ng cho vi·ªác x√¢y d·ª±ng c√°c gi·∫£i ph√°p t√≠ch h·ª£p AI t·∫°o sinh..." (https://genk.vn/vinbigdata-phat-trien-cong-nghe-ai-tao-sinh-se-som-cho-ra-mat-chatgpt-phien-ban-viet-20230821140700664.chn)
- Ng√†y 06 Nov-2023: PhoGPT: Generative Pre-training for Vietnamese (https://arxiv.org/abs/2311.02945)
- Ng√†y 05 Dec-2023: Th·∫ø gi·ªõi c√≥ ChatGPT, Vi·ªát Nam c√≥ Ph·ªüGPT (https://tuoitre.vn/the-gioi-co-chatgpt-viet-nam-co-phogpt-20231205164736363.htm)
- Ng√†y 06 Dec-2023: VinAI b·∫•t ng·ªù gi·ªõi thi·ªáu Ph·ªüGPT: C·∫°nh tranh v·ªõi ChatGPT, hi·ªÉu vƒÉn phong ti·∫øng Vi·ªát v∆∞·ª£t b·∫≠c (https://cafef.vn/vinai-bat-ngo-gioi-thieu-phogpt-canh-tranh-voi-chatgpt-hieu-van-phong-tieng-viet-vuot-bac-188231206132611864.chn)
- Ng√†y 06 Dec-2023: ƒê∆∞·ª£c m·ªánh danh l√† "ChatGPT phi√™n b·∫£n Vi·ªát", Ph·ªüGPT c√≥ g√¨ kh√°c bi·ªát? (https://doisongphapluat.nguoiduatin.vn/uoc-menh-danh-la-chat-gpt-phien-ban-viet-pho-gpt-co-gi-khac-biet-a395419.html)

# L√Ω do:
- Khi m√† tr√™n th·∫ø gi·ªõi tr√†n ng·∫≠p c√°c model English only v·ªõi quy m√¥ t·ª´ nh·ªè (1M-33M-124M-345M) ƒë·∫øn l·ªõn (7B-13B-34B) v√† kh·ªïng l·ªì (70B-180B), nh∆∞ng ·ªü Vi·ªát Nam th√¨ ch∆∞a th·∫•y c√≥ model n√†o quy m√¥ kho·∫£ng 7B d√πng ƒë∆∞·ª£c c·∫£ th√¨ vi·ªác fine-tuning ƒë∆∞·ª£c m·ªôt em ChatGPT bi·∫øt "ƒÉn n√≥i nh·∫π h√†ng" ·ªü nh√† ƒë·ªÉ v·ªçc v·∫°ch l√† "∆∞·ªõc m∆°" c·ªßa bao ƒë·ªùi L·∫≠p tr√¨nh vi√™n ch·ª© kh√¥ng c·ªßa ri√™ng ai. V√¨ v·∫≠y khi VinAi c√¥ng b·ªë model PhoGPT-7.5B l√† m√¨nh c≈©ng th·∫•y r·∫•t h√†o h·ª©ng v√† mu·ªën b·∫Øt tay v√†o fine-tuning em n√≥ ƒë·ªÉ ph·ª•c v·ª• m·ª•c ƒë√≠ch nghi√™n c·ª©u v√† h·ªçc t·∫≠p.
- M√¨nh chia s·∫ª k·∫øt qu·∫£ n√†y l√† ƒë·ªÉ ƒë·ªông vi√™n anh em "ng√†nh" h√£y c·ª© m·∫°nh d·∫°n Fine-tuning PhoGPT-7.5B (ho·∫∑c c√°c Model-7B n√≥i chung) b·∫±ng b·ªô th∆∞ vi·ªán PEFT-QLoRA th√¨ kh√° nh·∫π v√† nhanh ch·ª© kh√¥ng b·ªã n·∫∑ng v√† l√¢u ƒë√¢u nh√©. M·ªói item ch·ªâ train kho·∫£ng 1s tr√™n Colab T4 l√† qu√° ·ªïn nh√© v√¨ train b·∫±ng QLoRA n√™n s·ªë l∆∞·ª£ng params train r·∫•t √≠t m√† k·∫øt qu·∫£ v·∫´n Ok ^^

# Fine-tuning model PhoGPT-7.5B
- C·∫•u h√¨nh Colab T4 (15GB) tr·ªü l√™n
- Source code ‚Ä¶ kh√¥ng ph·∫£i l√†m g√¨ nhi·ªÅu v√¨ b·ªô th∆∞ vi·ªán Transformers v√† Peft n√≥ l√†m g·∫ßn h·∫øt vi·ªác r·ªìi, ch·ªâ m·ªói c·∫•u h√¨nh Trainer üòÇ
- Dataset c√° nh√¢n si√™u nh·ªè, tu·ª≥ nhu c·∫ßu s·ª≠ d·ª•ng
- Th·ªùi gian train ~1s/item
- File LoRA kh√° nh·ªè g·ªçn ~256MB, t·ªïng c·ªông t·∫•t c·∫£ c√°c files c·ªßa 1 checkpoint kho·∫£ng d∆∞·ªõi 800MB
- Training loss xu·ªëng c≈©ng kh√° nhanh, nh∆∞ng l·∫°i kh√¥ng b·ªã Overfit s·ªõm
- Khi d√πng Peft hay ·ªü ch·ªó l√† Fine-tuning th√¨ ch·ªâ c·∫ßn d√πng em T4-15GB colab (v√¨ d√πng QLoRA n√™n gi·∫£m ƒë∆∞·ª£c b·ªô nh·ªõ GPU), nh∆∞ng Inference l·∫°i ph·∫£i g·ªçi ƒë·∫øn em V100-16GB colab
- V√≠ d·ª•: n·∫øu doanh nghi·ªáp b·∫°n c√≥ kho·∫£ng 1000 c√¢u h·ªèi ƒë√°p, th√¨ dataset s·∫Ω l√† 1000 items, Fine-tuning kho·∫£ng 30 epoch ƒë·ªÉ cho em n√≥ overfit th√¨ 1000 items x 30 epochs = 30.000 iterations / 3600s = 8.33h Nvidia Tesla T4 colab x $0.42/h = $3.58 ~> m·ªôt chi ph√≠ qu√° ·ªïn cho m·ªôt em l·ªÖ t√¢n "c·ªßa nh√† tr·ªìng ƒë∆∞·ª£c" nh√© üòÇ

# Update 05/02/2024 01PM:
- khi c√°c b·∫°n train th√¨ nh·ªõ update c√°i prompt instruction theo sample_prompt_response_pairs (https://github.com/VinAIResearch/PhoGPT/blob/main/sample_instruction_following_dataset/sample_prompt_response_pairs.jsonl) thay cho c√°i prompt instruction m√¨nh ƒëang d√πng (version c≈©) nh√©:
- '### Instruction:{question}### Response:' ~> '### C√¢u h·ªèi:{question}\n\n### Tr·∫£ l·ªùi:'
- V√≠ d·ª•: "### C√¢u h·ªèi:\nT√¨m t·ª´ tr√°i nghƒ©a v·ªõi t·ª´ sau\n·ªín √†o\n\n### Tr·∫£ l·ªùi:Y√™n tƒ©nh."

# bnb_config = BitsAndBytesConfig(
- load_in_4bit=True,
- bnb_4bit_use_double_quant=True,
- bnb_4bit_quant_type="nf4",
- bnb_4bit_compute_dtype="float16")

# model = AutoModelForCausalLM.from_pretrained(
- mode_id,
- quantization_config=bnb_config,
- device_map="cua",
- trust_remote_code=True)

# peft_config = LoraConfig(
- r=32,
- lora_alpha=32,
- target_modules=["attn.Wqkv", "attn.out_proj", "ffn.up_proj", "ffn.down_proj"],
- lora_dropout=0.05,
- bias="none",
- task_type="CAUSAL_LM")

# training_arguments = TrainingArguments(
- output_dir=output_model,
- per_device_train_batch_size=1,
- gradient_accumulation_steps=1,
- optim="paged_adamw_32bit",
- learning_rate=1e-4,
- lr_scheduler_type="cosine",
- save_strategy="steps",
- save_steps=50,
- logging_steps=10,
- num_train_epochs=100,
- max_steps=100,
- fp16=True)

# trainer = SFTTrainer(
- model=model,
- train_dataset=data,
- peft_config=peft_config,
- dataset_text_field="text",
- args=training_arguments,
- tokenizer=tokenizer,
- packing=False,
- max_seq_length=1024)
