- PhoGPT-7.5B-Fine-tuning-QLoRA
- Author: Mr.Jack _ C√¥ng ty www.BICweb.vn
- Start Date: 23 Dec-2023
- End Date: 05 Feb-2024

# Foundation:
- github: https://github.com/VinAIResearch/PhoGPT
- huggingface: https://huggingface.co/vinai/PhoGPT-7B5-Instruct
- PhoGPT: Generative Pre-training for Vietnamese (https://arxiv.org/abs/2311.02945)
- MPT model: https://huggingface.co/docs/transformers/model_doc/mpt ; https://www.mosaicml.com/mpt
- Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs (https://www.databricks.com/blog/mpt-7b)
- PEFT: https://huggingface.co/docs/peft/en/index ; https://arxiv.org/abs/2205.05638
- LoRA: https://huggingface.co/docs/diffusers/en/training/lora ; https://arxiv.org/abs/2106.09685
- Quantization: https://huggingface.co/docs/accelerate/en/usage_guides/quantization
- Flash attention: https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention ; https://arxiv.org/abs/2205.14135
- ALiBi: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (https://arxiv.org/abs/2108.12409)

# S·ª± ki·ªán:
- Ng√†y 21 Aug-2023: "V√†o ng√†y 21/08/2023, C√¥ng ty C·ªï ph·∫ßn VinBigdata c√¥ng b·ªë x√¢y d·ª±ng th√†nh c√¥ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ti·∫øng Vi·ªát, ƒë·∫∑t n·ªÅn m√≥ng cho vi·ªác x√¢y d·ª±ng c√°c gi·∫£i ph√°p t√≠ch h·ª£p AI t·∫°o sinh..." (https://genk.vn/vinbigdata-phat-trien-cong-nghe-ai-tao-sinh-se-som-cho-ra-mat-chatgpt-phien-ban-viet-20230821140700664.chn)
- Ng√†y 06 Nov-2023: PhoGPT: Generative Pre-training for Vietnamese (https://arxiv.org/abs/2311.02945)
- Ng√†y 05 Dec-2023: Th·∫ø gi·ªõi c√≥ ChatGPT, Vi·ªát Nam c√≥ Ph·ªüGPT (https://tuoitre.vn/the-gioi-co-chatgpt-viet-nam-co-phogpt-20231205164736363.htm)
- Ng√†y 06 Dec-2023: VinAI b·∫•t ng·ªù gi·ªõi thi·ªáu Ph·ªüGPT: C·∫°nh tranh v·ªõi ChatGPT, hi·ªÉu vƒÉn phong ti·∫øng Vi·ªát v∆∞·ª£t b·∫≠c (https://cafef.vn/vinai-bat-ngo-gioi-thieu-phogpt-canh-tranh-voi-chatgpt-hieu-van-phong-tieng-viet-vuot-bac-188231206132611864.chn)
- Ng√†y 06 Dec-2023: ƒê∆∞·ª£c m·ªánh danh l√† "ChatGPT phi√™n b·∫£n Vi·ªát", Ph·ªüGPT c√≥ g√¨ kh√°c bi·ªát? (https://doisongphapluat.nguoiduatin.vn/uoc-menh-danh-la-chat-gpt-phien-ban-viet-pho-gpt-co-gi-khac-biet-a395419.html)

# L√Ω do:
- Khi m√† tr√™n th·∫ø gi·ªõi tr√†n ng·∫≠p c√°c model English only v·ªõi quy m√¥ t·ª´ nh·ªè (1M-33M-124M) ƒë·∫øn l·ªõn (7B-13B-33B) v√† kh·ªïng l·ªì (70B-180B), nh∆∞ng ·ªü Vi·ªát Nam th√¨ ch∆∞a th·∫•y c√≥ model n√†o quy m√¥ kho·∫£ng 7B d√πng ƒë∆∞·ª£c c·∫£ th√¨ vi·ªác fine-tuning ƒë∆∞·ª£c m·ªôt em ChatGPT bi·∫øt "ƒÉn n√≥i nh·∫π h√†ng" ·ªü nh√† ƒë·ªÉ v·ªçc v·∫°ch l√† "∆∞·ªõc m∆°" c·ªßa bao ƒë·ªùi L·∫≠p tr√¨nh vi√™n ch·ª© kh√¥ng c·ªßa ri√™ng ai. V√¨ v·∫≠y khi VinAi c√¥ng b·ªë model PhoGPT-7.5B l√† m√¨nh c≈©ng th·∫•y r·∫•t h√†o h·ª©ng v√† mu·ªën b·∫Øt tay v√†o fine-tuning em n√≥ ƒë·ªÉ ph·ª•c v·ª• m·ª•c ƒë√≠ch nghi√™n c·ª©u v√† h·ªçc t·∫≠p.
- M√¨nh chia s·∫ª k·∫øt qu·∫£ n√†y ƒë·ªÉ ƒë·ªông vi√™n anh em "ng√†nh" l√† anh em c·ª© m·∫°nh d·∫°n Fine-tuning PhoGPT-7.5B ho·∫∑c c√°c Model-7B n√≥i chung b·∫±ng b·ªô th∆∞ vi·ªán PEFT-QLoRA th√¨ kh√° nh·∫π v√† nhanh ch·ª© kh√¥ng b·ªã n·∫∑ng v√† l√¢u ƒë√¢u nh√©. M·ªói item ch·ªâ train kho·∫£ng 1s tr√™n Colab T4 l√† qu√° ·ªïn nh√© v√¨ train b·∫±ng QLoRA n√™n s·ªë l∆∞·ª£ng params train r·∫•t √≠t m√† k·∫øt qu·∫£ v·∫´n Ok ^^

# Fine-tuning model PhoGPT-7.5B
- C·∫•u h√¨nh Colab T4 (15GB) tr·ªü l√™n
- Source code ‚Ä¶ kh√¥ng ph·∫£i l√†m g√¨ nhi·ªÅu v√¨ b·ªô th∆∞ vi·ªán Transformers v√† Peft n√≥ l√†m g·∫ßn h·∫øt vi·ªác r·ªìi, ch·ªâ m·ªói c·∫•u h√¨nh Trainer üòÇ
- Dataset c√° nh√¢n si√™u nh·ªè, tu·ª≥ nhu c·∫ßu s·ª≠ d·ª•ng
- Th·ªùi gian train ~1s/item
- File LoRA kh√° nh·ªè g·ªçn, t·ªïng c·ªông kho·∫£ng d∆∞·ªõi 800MB
- Training loss xu·ªëng c≈©ng kh√° nhanh, nh∆∞ng l·∫°i kh√¥ng b·ªã Overfit s·ªõm
- Khi d√πng Peft hay ·ªü ch·ªó l√† Fine-tuning th√¨ ch·ªâ c·∫ßn d√πng em T4-15GB colab (v√¨ d√πng QLoRA n√™n gi·∫£m ƒë∆∞·ª£c b·ªô nh·ªõ GPU), nh∆∞ng Inference l·∫°i ph·∫£i g·ªçi ƒë·∫øn em V100-16GB colab üòÇ
