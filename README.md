- PhoGPT-7.5B-Fine-tuning-QLoRA
- Author: Mr.Jack _ CÃ´ng ty www.BICweb.vn
- Start Date: 23 Dec-2023
- End Date: 05 Feb-2024

# Foundation:
- github: https://github.com/VinAIResearch/PhoGPT
- huggingface: https://huggingface.co/vinai/PhoGPT-7B5-Instruct
- PhoGPT: Generative Pre-training for Vietnamese (https://arxiv.org/abs/2311.02945)
- MPT model: https://huggingface.co/docs/transformers/model_doc/mpt ; https://www.mosaicml.com/mpt
- Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs (https://www.databricks.com/blog/mpt-7b)
- PEFT: https://huggingface.co/docs/peft/en/index ; https://arxiv.org/abs/2205.05638
- LoRA: https://huggingface.co/docs/diffusers/en/training/lora ; https://arxiv.org/abs/2106.09685
- Quantization: https://huggingface.co/docs/accelerate/en/usage_guides/quantization
- Flash attention: https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention ; https://arxiv.org/abs/2205.14135
- ALiBi: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (https://arxiv.org/abs/2108.12409)

# Sá»± kiá»‡n:
- NgÃ y 21 Aug-2023: "VÃ o ngÃ y 21/08/2023, CÃ´ng ty Cá»• pháº§n VinBigdata cÃ´ng bá»‘ xÃ¢y dá»±ng thÃ nh cÃ´ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n tiáº¿ng Viá»‡t, Ä‘áº·t ná»n mÃ³ng cho viá»‡c xÃ¢y dá»±ng cÃ¡c giáº£i phÃ¡p tÃ­ch há»£p AI táº¡o sinh..." (https://genk.vn/vinbigdata-phat-trien-cong-nghe-ai-tao-sinh-se-som-cho-ra-mat-chatgpt-phien-ban-viet-20230821140700664.chn)
- NgÃ y 06 Nov-2023: PhoGPT: Generative Pre-training for Vietnamese (https://arxiv.org/abs/2311.02945)
- NgÃ y 05 Dec-2023: Tháº¿ giá»›i cÃ³ ChatGPT, Viá»‡t Nam cÃ³ Phá»ŸGPT (https://tuoitre.vn/the-gioi-co-chatgpt-viet-nam-co-phogpt-20231205164736363.htm)
- NgÃ y 06 Dec-2023: VinAI báº¥t ngá» giá»›i thiá»‡u Phá»ŸGPT: Cáº¡nh tranh vá»›i ChatGPT, hiá»ƒu vÄƒn phong tiáº¿ng Viá»‡t vÆ°á»£t báº­c (https://cafef.vn/vinai-bat-ngo-gioi-thieu-phogpt-canh-tranh-voi-chatgpt-hieu-van-phong-tieng-viet-vuot-bac-188231206132611864.chn)
- NgÃ y 06 Dec-2023: ÄÆ°á»£c má»‡nh danh lÃ  "ChatGPT phiÃªn báº£n Viá»‡t", Phá»ŸGPT cÃ³ gÃ¬ khÃ¡c biá»‡t? (https://doisongphapluat.nguoiduatin.vn/uoc-menh-danh-la-chat-gpt-phien-ban-viet-pho-gpt-co-gi-khac-biet-a395419.html)

# LÃ½ do:
- Khi mÃ  trÃªn tháº¿ giá»›i trÃ n ngáº­p cÃ¡c model English only vá»›i quy mÃ´ tá»« nhá» (1M-33M-124M) Ä‘áº¿n lá»›n (7B-13B-33B) vÃ  khá»•ng lá»“ (70B-180B), nhÆ°ng á»Ÿ Viá»‡t Nam thÃ¬ chÆ°a tháº¥y cÃ³ model nÃ o quy mÃ´ khoáº£ng 7B dÃ¹ng Ä‘Æ°á»£c cáº£ thÃ¬ viá»‡c fine-tuning Ä‘Æ°á»£c má»™t em ChatGPT biáº¿t "Äƒn nÃ³i nháº¹ hÃ ng" á»Ÿ nhÃ  Ä‘á»ƒ vá»c váº¡ch lÃ  "Æ°á»›c mÆ¡" cá»§a bao Ä‘á»i Láº­p trÃ¬nh viÃªn chá»© khÃ´ng cá»§a riÃªng ai. VÃ¬ váº­y khi VinAi cÃ´ng bá»‘ model PhoGPT-7.5B lÃ  mÃ¬nh cÅ©ng tháº¥y ráº¥t hÃ o há»©ng vÃ  muá»‘n báº¯t tay vÃ o fine-tuning em nÃ³ Ä‘á»ƒ phá»¥c vá»¥ má»¥c Ä‘Ã­ch nghiÃªn cá»©u vÃ  há»c táº­p.
- MÃ¬nh chia sáº» káº¿t quáº£ nÃ y lÃ  Ä‘á»ƒ Ä‘á»™ng viÃªn anh em "ngÃ nh" hÃ£y cá»© máº¡nh dáº¡n Fine-tuning PhoGPT-7.5B (hoáº·c cÃ¡c Model-7B nÃ³i chung) báº±ng bá»™ thÆ° viá»‡n PEFT-QLoRA thÃ¬ khÃ¡ nháº¹ vÃ  nhanh chá»© khÃ´ng bá»‹ náº·ng vÃ  lÃ¢u Ä‘Ã¢u nhÃ©. Má»—i item chá»‰ train khoáº£ng 1s trÃªn Colab T4 lÃ  quÃ¡ á»•n nhÃ© vÃ¬ train báº±ng QLoRA nÃªn sá»‘ lÆ°á»£ng params train ráº¥t Ã­t mÃ  káº¿t quáº£ váº«n Ok ^^

# Fine-tuning model PhoGPT-7.5B
- Cáº¥u hÃ¬nh Colab T4 (15GB) trá»Ÿ lÃªn
- Source code â€¦ khÃ´ng pháº£i lÃ m gÃ¬ nhiá»u vÃ¬ bá»™ thÆ° viá»‡n Transformers vÃ  Peft nÃ³ lÃ m gáº§n háº¿t viá»‡c rá»“i, chá»‰ má»—i cáº¥u hÃ¬nh Trainer ðŸ˜‚
- Dataset cÃ¡ nhÃ¢n siÃªu nhá», tuá»³ nhu cáº§u sá»­ dá»¥ng
- Thá»i gian train ~1s/item
- File LoRA khÃ¡ nhá» gá»n ~256MB, tá»•ng cá»™ng táº¥t cáº£ cÃ¡c files cá»§a 1 checkpoint khoáº£ng dÆ°á»›i 800MB
- Training loss xuá»‘ng cÅ©ng khÃ¡ nhanh, nhÆ°ng láº¡i khÃ´ng bá»‹ Overfit sá»›m
- Khi dÃ¹ng Peft hay á»Ÿ chá»— lÃ  Fine-tuning thÃ¬ chá»‰ cáº§n dÃ¹ng em T4-15GB colab (vÃ¬ dÃ¹ng QLoRA nÃªn giáº£m Ä‘Æ°á»£c bá»™ nhá»› GPU), nhÆ°ng Inference láº¡i pháº£i gá»i Ä‘áº¿n em V100-16GB colab
- VÃ­ dá»¥: náº¿u doanh nghiá»‡p cá»§a báº¡n cÃ³ khoáº£ng 1000 cÃ¢u há»i Ä‘Ã¡p, thÃ¬ dataset sáº½ lÃ  1000 items, Fine-tuning khoáº£ng 30 epoch Ä‘á»ƒ cho em nÃ³ overfit thÃ¬ 1000 items x 30 epochs = 30.000 iterations / 3600s = 8.33h Nvidia Tesla T4 colab x $0.42/h = $3.58 ~> má»™t chi phÃ­ quÃ¡ á»•n cho má»™t em lá»… tÃ¢n "cá»§a nhÃ  trá»“ng Ä‘Æ°á»£c" nhÃ© ðŸ˜‚

# bnb_config = BitsAndBytesConfig(
- load_in_4bit=True,
- bnb_4bit_use_double_quant=True,
- bnb_4bit_quant_type="nf4",
- bnb_4bit_compute_dtype="float16")

# model = AutoModelForCausalLM.from_pretrained(
- mode_id,
- quantization_config=bnb_config,
- device_map="cua",
- trust_remote_code=True)

# peft_config = LoraConfig(
- r=32,
- lora_alpha=32,
- target_modules=["attn.Wqkv", "attn.out_proj", "ffn.up_proj", "ffn.down_proj"],
- lora_dropout=0.05,
- bias="none",
- task_type="CAUSAL_LM")

# training_arguments = TrainingArguments(
- output_dir=output_model,
- per_device_train_batch_size=1,
- gradient_accumulation_steps=1,
- optim="paged_adamw_32bit",
- learning_rate=1e-4,
- lr_scheduler_type="cosine",
- save_strategy="steps",
- save_steps=50,
- logging_steps=10,
- num_train_epochs=100,
- max_steps=100,
- fp16=True)

# trainer = SFTTrainer(
- model=model,
- train_dataset=data,
- peft_config=peft_config,
- dataset_text_field="text",
- args=training_arguments,
- tokenizer=tokenizer,
- packing=False,
- max_seq_length=1024)
